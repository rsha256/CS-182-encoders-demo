{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, batch_size, splits=None, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          batch_size : number of samples per batch\n",
    "          splits : [train_frac, valid_frac]\n",
    "          shuffle : (bool)\n",
    "        \"\"\"\n",
    "        # flatten the images\n",
    "        self.transform = torchvision.transforms.Compose(\n",
    "            [torchvision.transforms.ToTensor(),\n",
    "             torchvision.transforms.Lambda(lambda x: x.view(-1))])       \n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.splits = splits\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self._build()\n",
    "      \n",
    "    def _build(self):\n",
    "        train_split, valid_split = self.splits\n",
    "        trainset = torchvision.datasets.MNIST(\n",
    "                root=\"data\", train=True, download=True, transform=self.transform)\n",
    "        num_samples = len(trainset)\n",
    "        self.num_train_samples = int(train_split * num_samples)\n",
    "        self.num_valid_samples = int(valid_split * num_samples)\n",
    "\n",
    "        # create training set \n",
    "        self.train_dataset = torch.utils.data.Subset(\n",
    "            trainset, range(0, self.num_train_samples))\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        \n",
    "        # create validation set\n",
    "        self.valid_dataset = torch.utils.data.Subset(\n",
    "            trainset, range(self.num_train_samples, num_samples))\n",
    "        self.valid_loader = torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "        # create test set\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                root=\"data\", train=False, download=True, transform=self.transform\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        self.num_test_samples = len(self.test_loader.dataset)\n",
    "\n",
    "    def get_num_samples(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            return self.num_train_samples\n",
    "        elif split == \"valid\":\n",
    "            return self.num_valid_samples\n",
    "        elif split == \"test\":\n",
    "            return self.num_test_samples\n",
    "\n",
    "    def get_batch(self, idx, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            return next(iter(self.train_loader))\n",
    "        elif split == \"valid\":\n",
    "            return next(iter(self.valid_loader))\n",
    "        elif split == \"test\":\n",
    "            return next(iter(self.test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder defines a general class of NN architectures\n",
    "           _____________                                     ___________\n",
    "          |             |                                   |           |                    \n",
    "    x --> | ENCODER (A) | --> z (latent representation) --> |DECODER (B)| --> x'\n",
    "          |_____________|                                   |___________|          \n",
    "\n",
    "    We implement a generic autoencoder with a fully connected encoder and decoder.\n",
    "    The encoder and decoder are defined by a list of hidden layer sizes. \n",
    "    Note that while this architecture is \"symmetric\" in dimensionality,\n",
    "    the encoder and decoder can have different architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.encoder = self._build_encoder()\n",
    "        self.decoder = self._build_decoder()\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        layers = []\n",
    "        prev_size = self.input_size\n",
    "        for layer_id, size in enumerate(self.hidden_sizes):\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            if layer_id < len(self.hidden_sizes)-1:\n",
    "                layers.append(self.activation())\n",
    "            prev_size = size\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        layers = []\n",
    "        prev_size = self.hidden_sizes[-1]\n",
    "        for size in reversed(self.hidden_sizes[:-1]):\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(self.activation())\n",
    "            prev_size = size\n",
    "        layers.append(nn.Linear(prev_size, self.input_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    def get_loss(self, x):\n",
    "        x_hat = self(x)\n",
    "        return self.loss(x, x_hat)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def loss(self, x, x_hat):\n",
    "        return nn.functional.mse_loss(x, x_hat)\n",
    "\n",
    "    def train_step(self, x, optimizer):\n",
    "        x_hat = self(x)\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked autoencoder\n",
    "class MaskedAutoencoder(Autoencoder):\n",
    "    def __init__(self, input_size, hidden_sizes, activation=nn.ReLU, mask_prob=0.25):\n",
    "        super().__init__(input_size, hidden_sizes, activation)\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def train_step(self, x, optimizer):\n",
    "        mask = torch.from_numpy(np.random.binomial(1, self.mask_prob, x.shape)).float()\n",
    "        x_masked = x * mask\n",
    "        x_hat = self(x_masked)\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled masked autoencoder\n",
    "class ScaledMaskedAutoencoder(Autoencoder):\n",
    "    def __init__(self, input_size, hidden_sizes, activation=nn.ReLU, mask_prob=0.25):\n",
    "        super().__init__(input_size, hidden_sizes, activation)\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def train_step(self, x, optimizer):\n",
    "        mask = torch.from_numpy(np.random.binomial(1, self.mask_prob, x.shape)).float()\n",
    "        x_masked = (1/self.mask_prob) * x * mask\n",
    "        x_hat = self(x_masked)\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/plotting utilities\n",
    "\n",
    "def plotter(train_losses, test_losses, savepath=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    MODELS = train_losses.keys()\n",
    "    num_models = len(MODELS)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, num_models))\n",
    "    for idx, model in enumerate(MODELS):\n",
    "        plt.plot(\n",
    "            train_losses[model],\n",
    "            linewidth=2,\n",
    "            label=model,\n",
    "            color=colors[idx])\n",
    "        plt.plot(\n",
    "            test_losses[model],\n",
    "            linewidth=2,\n",
    "            linestyle='-.',\n",
    "            label=model + \"_test\",\n",
    "            color=colors[idx])\n",
    "    plt.xlabel(\"#Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Avg. Loss\", fontsize=16)\n",
    "    plt.title(\"Comparing training w/ different AE architectures\", fontsize=24)\n",
    "    plt.legend(fontsize=14)\n",
    "    if savepath:\n",
    "        plt.savefig(savepath)\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, dataset, model, batch_size=100, num_classes=2):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=5e-4)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def train(self, num_epochs=100):\n",
    "        train_losses, valid_losses = [], []\n",
    "\n",
    "        pbar = tqdm(range(num_epochs))\n",
    "        num_batches = self.dataset.num_train_samples // self.batch_size\n",
    "\n",
    "        for epoch in pbar:\n",
    "            for batch_idx in range(num_batches):\n",
    "                x, y = self.dataset.get_batch(batch_idx, split=\"train\")\n",
    "                loss = self.model.train_step(x, self.optimizer)\n",
    "\n",
    "                # update progress\n",
    "                pbar.set_description(f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n",
    "\n",
    "            # evaluate loss on valid dataset\n",
    "            train_loss = self.get_loss(split=\"train\")\n",
    "            valid_loss = self.get_loss(split=\"valid\")\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "        return {\"train_losses\": train_losses, \"valid_losses\": valid_losses}\n",
    "\n",
    "    def get_loss(self, split=\"train\"):\n",
    "        num_samples = self.dataset.get_num_samples(split=split)\n",
    "        num_batches = num_samples // self.batch_size\n",
    "        losses = []\n",
    "        for batch_idx in range(num_batches):\n",
    "            x, y = self.dataset.get_batch(batch_idx, split=split)\n",
    "            loss = self.model.get_loss(x)\n",
    "            losses.append(loss.item())\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def get_model_accuracy(self, classifier, split=\"test\"):\n",
    "        \"\"\"\n",
    "        Compute the model accuracy with a linear classifer.\n",
    "        \"\"\"\n",
    "        num_samples, num_correct = 0, 0\n",
    "        num_batches = self.dataset.num_test_samples // self.batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            x, y = self.dataset.get_batch(batch_idx, split=\"test\")\n",
    "            z = self.model.encode(x)\n",
    "            y_hat = classifier(z)\n",
    "            preds = (y_hat.argmax(dim=1) == y).numpy()\n",
    "            num_samples += len(preds)\n",
    "            num_correct += np.sum(preds)\n",
    "        return num_correct / num_samples\n",
    "        \n",
    "    def evaluate_w_linear_probe(self, feats_dim, num_epochs=10):\n",
    "        # create a classifier\n",
    "        probe = nn.Linear(feats_dim, self.num_classes)\n",
    "\n",
    "        # setup optimizer\n",
    "        probe_opt = optim.Adam(probe.parameters(), lr=1e-3)\n",
    "\n",
    "        # train linear probe\n",
    "        # note that we train on a small subset of the labelled data \n",
    "        pbar = tqdm(range(num_epochs))\n",
    "        num_batches = self.dataset.num_valid_samples // self.batch_size\n",
    "\n",
    "        for epoch in pbar:\n",
    "            for batch_idx in range(num_batches):\n",
    "                x, y = self.dataset.get_batch(batch_idx, split=\"train\")\n",
    "                feat = self.model.encode(x)\n",
    "                y_hat = probe(feat)\n",
    "\n",
    "                # compute loss, optimize\n",
    "                loss = nn.functional.cross_entropy(y_hat, y)\n",
    "                probe_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                probe_opt.step()\n",
    "\n",
    "                # update progress\n",
    "                pbar.set_description(\n",
    "                    f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n",
    "        \n",
    "        # evaluate linear probe\n",
    "        accuracy = self.get_model_accuracy(classifier=probe)\n",
    "        return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MaskedAutoencoder(input_shape=784).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REPEATS = 5\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "##########################################\n",
    "####### TODO : define model architecture \n",
    "# hidden dims is a list of ints\n",
    "INPUT_DIMS = 784\n",
    "NUM_CLASSES = 10\n",
    "ACTIVATION = torch.nn.Sigmoid\n",
    "HIDDEN_DIMS = [100, 50, 5]\n",
    "##########################################\n",
    "\n",
    "# load and create MNIST dataset\n",
    "dataset = MNIST(BATCH_SIZE, [0.6, 0.4])\n",
    "\n",
    "# logging metrics\n",
    "feats_dim = HIDDEN_DIMS[-1]\n",
    "train_losses, test_losses = {}, {}\n",
    "accuracy = {}\n",
    "\n",
    "# run experiment w/ different models\n",
    "for model_idx, model_cls in MODELS.items():\n",
    "    _train_loss, _valid_loss, _acc = [], [], []\n",
    "    for expid in range(NUM_REPEATS):\n",
    "        print(\"run : {},  model : {}\".format(expid, model_idx))\n",
    "        model = model_cls(INPUT_DIMS, HIDDEN_DIMS, activation=ACTIVATION)\n",
    "        experiment = Experiment(dataset, model, num_classes=NUM_CLASSES)\n",
    "        train_stats = experiment.train(num_epochs=NUM_EPOCHS)\n",
    "        eval_stats = experiment.evaluate_w_linear_probe(feats_dim, NUM_EPOCHS)\n",
    "        _train_loss.append(train_stats[\"train_losses\"])\n",
    "        _valid_loss.append(train_stats[\"valid_losses\"])\n",
    "        _acc.append(eval_stats[\"accuracy\"])\n",
    "      \n",
    "    train_losses[model_idx] = np.mean(_train_loss, axis=0)\n",
    "    test_losses[model_idx] = np.mean(_valid_loss, axis=0)\n",
    "    accuracy[model_idx] = np.mean(_acc)\n",
    "\n",
    "# plot losses\n",
    "plotter(train_losses, test_losses)\n",
    "\n",
    "# report accuracy\n",
    "for model_idx, acc in accuracy.items():\n",
    "    print(\"Model : {}, Accuracy : {}\".format(model_idx, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Masking with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
